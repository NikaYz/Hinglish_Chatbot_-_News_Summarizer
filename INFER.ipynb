{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "xG3gbEn6zJeU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Hinglish Chatbot\n",
        "---\n"
      ],
      "metadata": {
        "id": "xhniEJ4C4E2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# clean_text function is used to convert text to lower and handle words such that:\n",
        "# -> if there are punctiation it would add space across it.\n",
        "# -> if there is some unnecessary character then it could convert it into space\n",
        "# -> Strip the sentence\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([@#'।,?!])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9@#'।,?!]+\", ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "with open('chatbot_tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "model = tf.keras.models.load_model('/content/chatbot_attention.keras')"
      ],
      "metadata": {
        "id": "oJOk0euSzb3c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tested on various inputs\n",
        "MAXLEN_QUESTIONS = 96\n",
        "MAXLEN_ANSWERS = 97\n",
        "# result from : '/content/chatbot_attention.keras'\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM,Concatenate, Embedding, Dense, Dot, Activation\n",
        "hidden_units = 512\n",
        "def build_inference_models(model, hidden_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer(\"lstm_6\").output\n",
        "    encoder_model = Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
        "\n",
        "    # Decoder Inputs\n",
        "    decoder_inputs = Input(shape=(1,), name=\"decoder_input_infer\")\n",
        "    decoder_state_input_h = Input(shape=(hidden_units,), name=\"decoder_h\")\n",
        "    decoder_state_input_c = Input(shape=(hidden_units,), name=\"decoder_c\")\n",
        "    encoder_outputs_input = Input(shape=(None, hidden_units), name=\"encoder_outputs_infer\")\n",
        "\n",
        "    # Decoder Embedding\n",
        "    decoder_embedding_layer = model.get_layer(\"embedding_8\")\n",
        "    decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "    # Decoder LSTM\n",
        "    decoder_lstm = model.get_layer(\"lstm_7\")\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        "    )\n",
        "\n",
        "    # Attention\n",
        "    score = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs_input])\n",
        "    attention_weights = Activation('softmax')(score)\n",
        "    context_vector = Dot(axes=[2, 1])([attention_weights, encoder_outputs_input])\n",
        "    decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
        "\n",
        "    # Dense Output\n",
        "    decoder_dense = model.get_layer(\"dense_3\")\n",
        "    decoder_outputs_final = decoder_dense(decoder_combined_context)\n",
        "\n",
        "    # Decoder Model\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs, encoder_outputs_input, decoder_state_input_h, decoder_state_input_c],\n",
        "        [decoder_outputs_final, state_h_dec, state_c_dec]\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "encoder_model, decoder_model = build_inference_models(model, hidden_units)\n",
        "\n",
        "\n",
        "def decode_sequence(input_text, tokenizer, maxlen_questions, maxlen_answers, temperature=0.8):\n",
        "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=maxlen_questions, padding='post')\n",
        "\n",
        "    # Run encoder\n",
        "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n",
        "    states_value = [state_h, state_c]\n",
        "\n",
        "    start_token = tokenizer.word_index.get(\"<start>\")\n",
        "    end_token = tokenizer.word_index.get(\"<end>\")\n",
        "    target_seq = np.array([[start_token]])\n",
        "\n",
        "    decoded_sentence = []\n",
        "\n",
        "    for _ in range(maxlen_answers):\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq, encoder_outputs] + states_value\n",
        "        )\n",
        "\n",
        "        output_distribution = output_tokens[0, -1, :]\n",
        "        #output_distribution = np.log(output_distribution + 1e-10) / temperature\n",
        "        output_distribution = np.log(output_distribution + 1e-10) / temperature\n",
        "        exp_preds = np.exp(output_distribution)\n",
        "        output_distribution = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "\n",
        "        sampled_token_index = np.argmax(output_distribution)\n",
        "        #sampled_token_index = np.random.choice(len(output_distribution), p=output_distribution)\n",
        "\n",
        "        sampled_word = tokenizer.index_word.get(sampled_token_index, \"?\")\n",
        "\n",
        "        if sampled_token_index == end_token or sampled_word == \"?\":\n",
        "            break\n",
        "\n",
        "        decoded_sentence.append(sampled_word)\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return \" \".join(decoded_sentence)\n",
        "\n",
        "input_texts = [\n",
        "    \"Hey Radhika! Kaisi ho?\",\n",
        "    \"Mujhe teri kuch paintings dekhni hai. Kya hum ek din art exhibition pe milke chal sakte hai?\",\n",
        "    \"tumhara naam kya hai?\",\n",
        "    \"aaj mausam kaisa hai?\",\n",
        "    \"kahan ja rahe ho?\"\n",
        "]\n",
        "\n",
        "# Iterate through the list of input texts\n",
        "for i, input_text in enumerate(input_texts, 1):\n",
        "    input_text = clean_text(input_text)\n",
        "    print(f\"User (Input {i}): {input_text}\")\n",
        "    response = decode_sequence(input_text, tokenizer, MAXLEN_QUESTIONS, MAXLEN_ANSWERS)\n",
        "    print(f\"Bot (Response {i}): {response}\\n\")\n"
      ],
      "metadata": {
        "id": "v_TiEqSbzTQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752c3285-74f8-47b2-f943-244b7e28010b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User (Input 1): hey radhika ! kaisi ho ?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Bot (Response 1): main bhi theek hoon , bas thoda busy ho rahi hoon\n",
            "\n",
            "User (Input 2): mujhe teri kuch paintings dekhni hai kya hum ek din art exhibition pe milke chal sakte hai ?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "Bot (Response 2): haan , mujhe bhi bahut pasand hai ! aur mujhe bhi pasand hai ki hum dono ek dusre ke saath saath time spend karenge\n",
            "\n",
            "User (Input 3): tumhara naam kya hai ?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "Bot (Response 3): haan , mujhe bhi tumse pyaar ho gaya hai\n",
            "\n",
            "User (Input 4): aaj mausam kaisa hai ?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "Bot (Response 4): haan , mujhe bhi wahi feeling ho rahi hai\n",
            "\n",
            "User (Input 5): kahan ja rahe ho ?\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Bot (Response 5): haan , bilkul ! main bhi tumse pyaar karti hoon\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "NEWS SUMMARIZER\n",
        "---"
      ],
      "metadata": {
        "id": "vtxEoG-rDA9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"By . Associated Press . PUBLISHED: . 14:11 EST, 25 October 2013 . | . UPDATED: . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . State Immunization Program Manager Molly Howell says the risk is low, but officials feel it's important to alert people to the possible exposure. The diocese announced on Monday that Bishop John Folda is taking time off after being diagnosed with hepatitis A. The diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in Italy last month. Symptoms of hepatitis A include fever, tiredness, loss of appetite, nausea and abdominal discomfort. Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located .\"\n",
        "import re\n",
        "contractions_map =  {\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"i've\": \"i have\",\n",
        "    \"you've\": \"you have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "}\n",
        "with open('summary_tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "def clean_text(raw_text):\n",
        "\n",
        "    def expand_contractions(text, contractions_map):\n",
        "        if not contractions_map:\n",
        "            return text\n",
        "        pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in contractions_map.keys()) + r')\\b',\n",
        "                             flags=re.IGNORECASE)\n",
        "        return pattern.sub(lambda m: contractions_map.get(m.group(0).lower(), m.group(0)), text)\n",
        "\n",
        "    # apply strop and lowering the text\n",
        "    text = re.sub(r'\\s+', ' ', raw_text).strip().lower()\n",
        "    # make changes for short form words to full like isn't to is not\n",
        "    text = expand_contractions(text, contractions_map)\n",
        "\n",
        "    text = re.sub(r\"\\b(?!(\" + \"|\".join(re.escape(k) for k in contractions_map) + r\"))(\\w+)'s\\b\", r\"\\2\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # remove published and updated words from the starting\n",
        "    text = re.sub(r'By\\s+.*?PUBLISHED:.*?UPDATED:.*?\\.\\s*', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # remove  \"By . Associated Press . PUBLISHED\"\n",
        "    text = re.sub(r'By\\s+\\.?\\s*[A-Za-z\\s]*\\.?\\s*PUBLISHED:.*?UPDATED:.*?\\.\\s*', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'^By\\s+\\.*\\s*[a-z\\s]+\\.?\\s*', '', text, flags=re.IGNORECASE)\n",
        "    # remove date  and time stamp\n",
        "    text = re.sub(r'\\d{1,2}:\\d{2}\\s*[A-Z]{2,4},\\s*\\d{1,2}\\s+\\w+\\s+\\d{4}\\s*\\.', '', text,  flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b(?:last\\s+updated\\s+at\\s+)?\\d{1,2}:\\d{2}\\s*[APap][Mm]\\s+on\\s+\\d{1,2}(?:st|nd|rd|th)?\\s+\\w+\\s+\\d{4}\\s*,?', '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # if any part is there in brackets we don't consider it for highlights\n",
        "    text = re.sub(r'\\s*[\\(\\[].*?[\\)\\]]', '', text)\n",
        "\n",
        "    # there were this words like video and read more which is unnecessary for highlight creation\n",
        "    text = re.sub(r'scroll down for video.*?(?=\\s[a-z])', '', text)\n",
        "    text = re.sub(r'watch the video above.*?(?=\\s[a-z])', '', text)\n",
        "    text = re.sub(r'read more:.*?(?=\\s[a-z])', '', text)\n",
        "\n",
        "    # replace unnecessary punctuation marks like repetititve\n",
        "    text = re.sub(r'--+', ' ', text)\n",
        "    text = re.sub(r'[“”\"]', '', text)\n",
        "    text = re.sub(r\"[^\\w\\s.,?!$£€₹\\-']\", '', text)\n",
        "\n",
        "    # replace multiple commas and dots to single\n",
        "    text = re.sub(r'\\.{2,}', '.', text)\n",
        "    text = re.sub(r',{2,}', ',', text)\n",
        "    text = re.sub(r'^\\.\\s*', '', text)\n",
        "    # remove whitespave from start and end\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "input = clean_text(input)\n",
        "# preparing articles and highlights for training\n",
        "max_len_articles = 400\n",
        "max_len_highlights = 199\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "\n",
        "tokenized_articles = tokenizer.texts_to_sequences([input])\n",
        "\n",
        "# padding and truncating both articles and highlights\n",
        "padded_articles = pad_sequences(\n",
        "    tokenized_articles,\n",
        "    maxlen=max_len_articles,\n",
        "    padding='post',\n",
        "    truncating='post'\n",
        ")\n",
        "\n",
        "\n",
        "encoder_input_data = np.array(padded_articles, dtype=np.int32)\n",
        "\n",
        "print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model('/content/summary_lstm.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgLsBEbl5WOD",
        "outputId": "619ca118-9a02-4738-d625-334d6a3f586d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Input Shape: (1, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hidden_units = 256\n",
        "MAXLEN_ANSWERS = 199\n",
        "MAXLEN_QUESTIONS = 400\n",
        "def build_inference_models(model, hidden_units):\n",
        "    # Encoder\n",
        "    encoder_inputs = model.input[0]\n",
        "    encoder_outputs, state_h_enc, state_c_enc = model.get_layer(\"lstm_4\").output\n",
        "    encoder_model = Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
        "\n",
        "    # Decoder Inputs\n",
        "    decoder_inputs = Input(shape=(1,), name=\"decoder_input_infer\")\n",
        "    decoder_state_input_h = Input(shape=(hidden_units,), name=\"decoder_h\")\n",
        "    decoder_state_input_c = Input(shape=(hidden_units,), name=\"decoder_c\")\n",
        "    encoder_outputs_input = Input(shape=(None, hidden_units), name=\"encoder_outputs_infer\")\n",
        "\n",
        "    # Decoder Embedding\n",
        "    decoder_embedding_layer = model.get_layer(\"embedding_3\")\n",
        "    decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "    # Decoder LSTM\n",
        "    decoder_lstm = model.get_layer(\"lstm_5\")\n",
        "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "        decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        "    )\n",
        "\n",
        "    # Attention\n",
        "    score = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs_input])\n",
        "    attention_weights = Activation('softmax')(score)\n",
        "    context_vector = Dot(axes=[2, 1])([attention_weights, encoder_outputs_input])\n",
        "    decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
        "\n",
        "    # Dense Output\n",
        "    decoder_dense = model.get_layer(\"dense_2\")\n",
        "    decoder_outputs_final = decoder_dense(decoder_combined_context)\n",
        "\n",
        "    # Decoder Model\n",
        "    decoder_model = Model(\n",
        "        [decoder_inputs, encoder_outputs_input, decoder_state_input_h, decoder_state_input_c],\n",
        "        [decoder_outputs_final, state_h_dec, state_c_dec]\n",
        "    )\n",
        "\n",
        "    return encoder_model, decoder_model\n",
        "encoder_model, decoder_model = build_inference_models(model, hidden_units)\n",
        "\n",
        "\n",
        "def decode_sequence(input_text, tokenizer, maxlen_questions, maxlen_answers, temperature=1):\n",
        "    input_seq = encoder_input_data[0].reshape(1,maxlen_questions)  # shape = (1, 400)\n",
        "\n",
        "    # Run encoder\n",
        "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n",
        "    states_value = [state_h, state_c]\n",
        "\n",
        "    start_token = tokenizer.word_index.get(\"startseq\")\n",
        "    end_token = tokenizer.word_index.get(\"endseq\")\n",
        "    target_seq = np.array([[start_token]])\n",
        "\n",
        "    decoded_sentence = []\n",
        "\n",
        "    for _ in range(maxlen_answers):\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq, encoder_outputs] + states_value\n",
        "        )\n",
        "\n",
        "        output_distribution = output_tokens[0, -1, :]\n",
        "        #output_distribution = np.log(output_distribution + 1e-10) / temperature\n",
        "        output_distribution = np.log(output_distribution + 1e-10) / temperature\n",
        "        exp_preds = np.exp(output_distribution)\n",
        "        output_distribution = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "\n",
        "        sampled_token_index = np.argmax(output_distribution)\n",
        "        #sampled_token_index = np.random.choice(len(output_distribution), p=output_distribution)\n",
        "\n",
        "        sampled_word = tokenizer.index_word.get(sampled_token_index, \"?\")\n",
        "\n",
        "        if sampled_token_index == end_token or sampled_word == \"?\":\n",
        "            break\n",
        "\n",
        "        decoded_sentence.append(sampled_word)\n",
        "        target_seq = np.array([[sampled_token_index]])\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return \" \".join(decoded_sentence)\n",
        "\n",
        "input_seq = encoder_input_data[0]\n",
        "\n",
        "print(input)\n",
        "response = decode_sequence(input_seq, tokenizer, MAXLEN_QUESTIONS, MAXLEN_ANSWERS)\n",
        "print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1SU8Sut6IdH",
        "outputId": "800b7357-b2bf-4a4b-90c0-d30e80ee5202"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the bishop of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a virus in late september and early october. the state health department has issued an advisory of exposure for anyone who attended five churches and took communion. bishop john folda of the fargo catholic diocese in north dakota has exposed potentially hundreds of church members in fargo, grand forks and jamestown to the hepatitis a . state immunization program manager molly howell says the risk is low, but officials feel it is important to alert people to the possible exposure. the diocese announced on monday that bishop john folda is taking time off after being diagnosed with hepatitis a. the diocese says he contracted the infection through contaminated food while attending a conference for newly ordained bishops in italy last month. symptoms of hepatitis a include fever, tiredness, loss of appetite, nausea and abdominal discomfort. fargo catholic diocese in north dakota is where the bishop is located .\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "Bot: bishop of the bishop of the bank catholic bishop john duggan says the risk is low but officials say bishop john bercow of the bank of the bank of the victims of the health health department has issued an advisory of exposure for anyone who attended five church and took place\n"
          ]
        }
      ]
    }
  ]
}